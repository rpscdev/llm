{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/rpscdev/llm/blob/main/Gemini_RAG.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Install the following library for the langchanin gemini gradio\n",
        "!pip install -q -U langchain langchain-google-genai faiss-cpu gradio langchain-community google-generativeai google-ai-generativelanguage"
      ],
      "metadata": {
        "id": "ojyRzvo2J4bD",
        "outputId": "f384bbef-939a-481a-a707-57b750989a6e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/42.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.3 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m66.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import gradio as gr\n",
        "\n",
        "# LangChain specific imports for Google Gemini\n",
        "from langchain_community.document_loaders import TextLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_community.vectorstores import FAISS\n",
        "from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI\n",
        "from langchain.chains import RetrievalQA\n",
        "\n",
        "# Api key fetching from secret tab\n",
        "GOOGLE_API_KEY = os.environ.get(\"GOOGLE_API_KEY\")\n",
        "# Set it as an environment variable for LangChain\n",
        "\n",
        "#Sample Story text for Demo\n",
        "document_content = \"\"\"\n",
        "A lion was once sleeping in the jungle when a mouse started running up and down his body just for fun.\n",
        "This disturbed the lion’s sleep, and he woke up quite angry. He was about to eat the mouse when the mouse\n",
        "desperately requested the lion to set him free. “I promise you, I will be of great help to you someday if you\n",
        "save me.” The lion laughed at the mouse’s confidence and let him go. One day, a six-seven hunters came into the forest\n",
        "and took the lion with them. They tied him up against a tree. The lion was struggling to get out and started to whimper.\n",
        "Soon, the mouse walked past and noticed the lion in trouble. Quickly, he ran and gnawed on the ropes to set the lion free.\n",
        "Both of them sped off into the jungle.\n",
        "\"\"\"\n",
        "\n",
        "# Save the content to a temporary file for TextLoader to process\n",
        "temp_file_path = \"temp_document.txt\"\n",
        "with open(temp_file_path, \"w\") as f:\n",
        "    f.write(document_content)\n",
        "\n",
        "print(f\"Document content saved to '{temp_file_path}' for processing.\")\n",
        "\n",
        "# step 1. Load and Split your text file (from the temporary file) ---\n",
        "loader = TextLoader(temp_file_path)\n",
        "documents = loader.load()\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
        "chunks = splitter.split_documents(documents)\n",
        "\n",
        "# step 2. Create vector store with Gemini embeddings ---\n",
        "print(\"Creating vector store with Gemini embeddings...\")\n",
        "embedding = GoogleGenerativeAIEmbeddings(model=\"models/embedding-001\")\n",
        "vectorstore = FAISS.from_documents(chunks, embedding)\n",
        "print(\"Vector store created.\")\n",
        "\n",
        "# step 3. Setup retriever and Gemini LLM ---\n",
        "print(\"Setting up Gemini LLM and RAG chain...\")\n",
        "retriever = vectorstore.as_retriever()\n",
        "llm = ChatGoogleGenerativeAI(model=\"gemini-2.0-flash\", temperature=0.2)\n",
        "qa_chain = RetrievalQA.from_chain_type(llm=llm, retriever=retriever)\n",
        "print(\"RAG chain ready.\")\n",
        "\n",
        "# step 4. Define Gradio QnA function ---\n",
        "def answer_question(query):\n",
        "    try:\n",
        "        # LangChain's RetrievalQA chain uses 'invoke' for new versions\n",
        "        result = qa_chain.invoke({\"query\": query})\n",
        "        return result.get('result', 'No direct answer found.') # Access 'result' key from the dictionary\n",
        "    except Exception as e:\n",
        "        return f\"⚠️ Error: {str(e)}\"\n",
        "\n",
        "# --- Gradio interface ---\n",
        "print(\"Launching Gradio interface...\")\n",
        "gr.Interface(\n",
        "    fn=answer_question,\n",
        "    inputs=gr.Textbox(label=\"Ask a question about the Story\", placeholder=\"E.g., Type you Query?\"),\n",
        "    outputs=gr.Textbox(label=\"Gemini Answer\"),\n",
        "    title=\"Gemini-Powered RAG Q&A\",\n",
        "    description=\"Ask questions about the provided Story using Google Gemini and LangChain.\",\n",
        "    allow_flagging=\"never\" # Optional: disable flagging\n",
        ").launch(debug=True) # debug=True can help with troubleshooting\n",
        "print(\"Gradio interface launched!\")"
      ],
      "metadata": {
        "id": "borwEZroI5Bf",
        "outputId": "99d3ef16-ccee-4f07-d734-66e256d87a18",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 784
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Document content saved to 'temp_document.txt' for processing.\n",
            "Creating vector store with Gemini embeddings...\n",
            "Vector store created.\n",
            "Setting up Gemini LLM and RAG chain...\n",
            "RAG chain ready.\n",
            "Launching Gradio interface...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gradio/interface.py:419: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "It looks like you are running Gradio on a hosted a Jupyter notebook. For the Gradio app to work, sharing must be enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "* Running on public URL: https://52ce7a4d41e57ad7e4.gradio.live\n",
            "\n",
            "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://52ce7a4d41e57ad7e4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}